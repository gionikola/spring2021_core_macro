{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggett Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents:\n",
    "\n",
    "- [Huggett Model](#Huggett-Model)  \n",
    "  - [Model Overview](#Model-Overview) \n",
    "  - [Solving for a Stationary Equilibrium](#Solving-for-a-Stationary-Equilibrium) \n",
    "  - [Analysis](#Analysis)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab includes:\n",
    "\n",
    "(1) A brief overview of the Huggett's incomplete markets model;\n",
    "\n",
    "(2) A computational approach to solving for stationary equilibria for Huggett's model;\n",
    "\n",
    "(3) Analyses of changes in the stationary equilibrium by varying the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggett (1993) sets up and analyzes a pure consumption-loans economy. \n",
    "\n",
    "The economy is populated by a unit mass of households.\n",
    "\n",
    "Each household lives to maximize their lifetime utility, which is a function of its consumption path.\n",
    "\n",
    "In each period, each household is endowed stochastically and idiosyncratically with some income.\n",
    "\n",
    "More specifically, the household's endowment follows a Markov chain $(\\mathcal{P},\\overline{s})$.\n",
    "\n",
    "The stochastic and idiosyncratic nature of the endowment process in the economy creates risk for each household.\n",
    "\n",
    "The existence of risk creates the need for risk management / insurance against the risk.\n",
    "\n",
    "The vehicle for such insurance is the centralized loan market, where households can borrow and lend at some constant risk-free interest rate $r$.\n",
    "\n",
    "Total borrowing is bounded by some $\\phi > 0$ (otherwise debt levels could blow up).\n",
    "\n",
    "$\\ast$ The level of assets is restricted to the *discrete* set $\\mathcal{A} = [\\overline{a}_1, \\ldots, \\overline{a}_m]$, where the lower bound on assets is $\\overline{a}_1 = - \\phi$.\n",
    "\n",
    "(Why does $\\overline{a}_1 = - \\phi$ make sense? Think abut the intuitive relationship between assets and debt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary Equilibrium Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a borrowing limit $\\phi$ a **stationary equilibrium** is an interest rate $r$, a policy function $g(a,s)$, and a stationary distribution $\\lambda(a,s)$ for which "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) \n",
    "\n",
    "Given $r$, the policy function $g(a,s)$ solves the household's optimum problem\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{ \\{c_t,a_{t+1}\\} } E_0 \\sum_{t=0}^\\infty \\beta^t u(c_t) \\, ,\n",
    "\\end{align*}\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "c_t + a_{t+1} &= (1+r) a_t + w s_t \\, , \\\\\n",
    "a_{t+1} &\\in \\mathcal{A} \\, ,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\beta \\in (0,1)$, $u(c)$ has the usual properties, and $\\beta (1+r) < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "The probability distribution $\\lambda(a,s)$ is the invariant distribution of the Markov chain on $(a,s)$ induced by the Markov chain $(\\mathcal{P},\\overline{s})$ and the optimal policy $g(a,s)$.\n",
    "\n",
    "Let's think a bit about this Markov chain. \n",
    "\n",
    "Define $x = (a,s)$ as a single state variable. \n",
    "\n",
    "More specifically, if $i = 1,\\ldots,n$ indexes the asset level grid $\\mathcal{A}$, and $h = 1,\\ldots,m$ indexes the possible income states $\\{ws_1,\\ldots,ws_m\\}$, then the $j$-th element of $x$ is is $x_j = (a_i, s_i)$, where $j= (i-1)m + h$:\n",
    "\n",
    "\\begin{align*}\n",
    "x' = [(\\overline{a}_1,\\overline{s}_1),\\ldots,(\\overline{a}_1,\\overline{s}_m),\n",
    "(\\overline{a}_2,\\overline{s}_1),\\ldots,(\\overline{a}_2,\\overline{s}_m),\n",
    "\\ldots, (\\overline{a}_n,\\overline{s}_1),\\ldots,(\\overline{a}_n,\\overline{s}_m)] \\, .\n",
    "\\end{align*}\n",
    "\n",
    "The Markov chain for $x$ is determined by the following equation:\n",
    "\n",
    "\\begin{align*}\n",
    "P[(a_{t+1} = a', s_{t+1} =s') \\,|\\, (a_t = a, s_t = s) ] \n",
    "&= \n",
    "P(a_{t+1} = a\\prime \\,|\\, a_t =a, s_t = s) \\cdot P(s_{t+1} = s' \\,|\\, s_t = s)\n",
    "\\\\\n",
    "&=\n",
    "    \\mathcal{I}(a\\prime, a,s) \\mathcal{P}(s,s') \\, ,\n",
    "\\end{align*}\n",
    "where $ \\mathcal{I}(a\\prime, a,s) = 1$ if $a' = g(a,s)$ and zero otherwise, and $\\mathcal{P}$ is the given Markov chain on $s$.\n",
    "\n",
    "The Markov process defined by the above equation corresponds to a $(n \\cdot m) \\times (n \\cdot m)$ transition matrix $P$.\n",
    "\n",
    "The stationary distribution corresponding to the transition matrix $P$ is a vector $\\pi_\\infty$ with $n \\cdot m$ entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)\n",
    "\n",
    "When $\\lambda(a,s)$ describes the cross-section of households at each date, the loan market clears:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{a,s} \\lambda(a,s) g(a,s) = 0 \\, .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solving for a Stationary Equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:53:41.789000-07:00",
     "start_time": "2021-05-25T00:53:38.479Z"
    }
   },
   "source": [
    "First we load all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T22:28:54.259000-07:00",
     "start_time": "2021-05-26T05:27:53.277Z"
    }
   },
   "outputs": [],
   "source": [
    "using Parameters\n",
    "using QuantEcon\n",
    "using LinearAlgebra\n",
    "using SparseArrays\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a structure called `Agent` that will contain all agent-specific model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:55:30.772000-07:00",
     "start_time": "2021-05-25T00:55:29.742Z"
    }
   },
   "outputs": [],
   "source": [
    "mutable struct Agent\n",
    "  σ::Float64 # risk aversion parameter\n",
    "  β::Float64 # discount factor\n",
    "  r::Float64 # interest rate \n",
    "  w::Float64 # endowment\n",
    "  e::Vector{Float64} # vector of productivities\n",
    "  π::Vector{Float64} # probability weights on productivities\n",
    "  agrid::Vector{Float64} # grid for asset holdings \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct a function `utility` that will take agent parameters and the level of consumption as inputs, and return the utility level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:55:41.117000-07:00",
     "start_time": "2021-05-25T00:55:41.112Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility function for agent when consumption is a real number\n",
    "\"\"\"\n",
    "function utility(agent::Agent,c::Real)\n",
    "    \n",
    "    σ = agent.σ # Store risk-aversion parameter\n",
    "    \n",
    "    # Define the utility mapping \n",
    "    # condition on the input `c`\n",
    "    if c > 0\n",
    "        if σ == 1\n",
    "          return log(c) # if σ = 1, CRRA utility is log\n",
    "        else\n",
    "          return c^(1-σ)/(1-σ) # if σ != 1, regular specification\n",
    "        end\n",
    "    else\n",
    "        return -Inf # infinite disutility if c∈(-∞,0]\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow for `utility` to be applied to a vector of different consumption levels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:55:51.837000-07:00",
     "start_time": "2021-05-25T00:55:51.838Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility function when consumption is a vector\n",
    "\"\"\"\n",
    "function utility(agent::Agent,cvec)\n",
    "  u(c) = utility(agent,c)\n",
    "  return u.(cvec)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function `iterateBellman` that applies a single Bellman mapping to an initial value function guess.\n",
    "As always, the model parameters must also be inputted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:56:01.844000-07:00",
     "start_time": "2021-05-25T00:56:01.759Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  iterateBellman(agent::Agent,Vcont)\n",
    "\n",
    "Iterates on the bellman equation using continuation value function Vcont\n",
    "\"\"\"\n",
    "function iterateBellman(agent::Agent,Vcont)\n",
    "  @unpack σ,agrid,π,w,e,r,β = agent #unpack parameters of agent\n",
    "  u(c) = utility(agent,c)#shorthand for utility\n",
    "  Na = length(agrid)\n",
    "  S = length(e)\n",
    "\n",
    "  EVcont = Vcont*π #precompute expected value\n",
    "\n",
    "  #solve for each state\n",
    "  #preallocate space for V and a_policy\n",
    "  V = similar(Vcont)\n",
    "  a_policy = similar(Vcont,Int)\n",
    "  for s  in 1:S\n",
    "    for ia_ in 1:Na\n",
    "      cvec = e[s]*w + (1+r)*agrid[ia_] - agrid #possible consumption values for each asset position\n",
    "      obj = u(cvec) + β * EVcont #objective for each asset position\n",
    "\n",
    "      V[ia_,s],a_policy[ia_,s] = findmax(obj) #choose maximum asset position\n",
    "    end\n",
    "  end\n",
    "  return V,a_policy\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:56:12.037000-07:00",
     "start_time": "2021-05-25T00:56:12.019Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  iterateBellman_alt(agent::Agent,Vcont)\n",
    "\n",
    "Iterates on the bellman equation using continuation value function Vcont.  Exploits\n",
    "that a_policy in increasing in a and that the value function is concave.\n",
    "\"\"\"\n",
    "function iterateBellman_alt(agent::Agent,Vcont)\n",
    "  @unpack σ,agrid,π,w,e,r,β = agent\n",
    "  u(c) = utility(agent,c)\n",
    "  Na = length(agrid)\n",
    "  S = length(e)\n",
    "  EVcont = Vcont*π #precompute expected value\n",
    "\n",
    "  EVcont = Vcont*π #precompute expected value\n",
    "\n",
    "  #solve for each state\n",
    "  #preallocate space for V and a_policy\n",
    "  V = similar(Vcont) #creates V the same size as Vcont\n",
    "  a_policy = similar(Vcont,Int) #note a_policy needs to be integers\n",
    "  for s  in 1:S\n",
    "    aprev = 1 #stores for the last ia_ the optimal policy\n",
    "    for ia_ in 1:Na\n",
    "      maxobj = -Inf #stores the maxobj up to this point\n",
    "      for ia in aprev:Na #start iterating at a_policy[ia_-1,s]\n",
    "        c = w*e[s] + (1+r)*agrid[ia_] - agrid[ia] #get consumption for this ia\n",
    "        obj = u(c) + β * EVcont[ia] #get objective\n",
    "        if obj >= maxobj #check if increasing ia improves utility\n",
    "          V[ia_,s],a_policy[ia_,s] = obj,ia #if so store values\n",
    "          maxobj = obj#and update maxobj\n",
    "        else\n",
    "          break #if it decreases utility end for loop, we have found our max\n",
    "        end\n",
    "      end\n",
    "      aprev = a_policy[ia_,s]#store the max in aprev for next ia_\n",
    "    end\n",
    "  end\n",
    "  return V,a_policy\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function `solveBellman` that applies the Bellman mapping iteratively until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:56:23.746000-07:00",
     "start_time": "2021-05-25T00:56:23.741Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  solveBellman(agent::Agent,V0,ϵ=1e-6)\n",
    "\n",
    "Solves the bellman equation for a given guess of V0 using iterateBellman\n",
    "\"\"\"\n",
    "function solveBellman(agent::Agent,V0,ϵ=1e-6)\n",
    "  Vcont = V0\n",
    "  a_policy = similar(V0,Int)\n",
    "  diff = 1.\n",
    "\n",
    "  while diff > ϵ\n",
    "    V,a_policy = iterateBellman(agent,Vcont)\n",
    "    diff = norm((V-Vcont)[:],Inf)\n",
    "    Vcont = V\n",
    "  end\n",
    "  return Vcont,a_policy\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:56:31.827000-07:00",
     "start_time": "2021-05-25T00:56:31.819Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  solveBellman_alt(agent::Agent,V0,ϵ=1e-6)\n",
    "\n",
    "Solves the bellman equation for a given guess of V0 using iterateBellman_alt\n",
    "\"\"\"\n",
    "function solveBellman_alt(agent::Agent,V0,ϵ=1e-6)\n",
    "  Vcont = V0\n",
    "  a_policy = similar(V0,Int)\n",
    "  diff = 1.\n",
    "\n",
    "  while diff > ϵ\n",
    "    V,a_policy = iterateBellman_alt(agent,Vcont)\n",
    "    diff = norm((V-Vcont)[:],Inf)\n",
    "    Vcont = V\n",
    "  end\n",
    "  return Vcont,a_policy\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function `constructTransitionMatrix` that... constructs the transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:56:42.490000-07:00",
     "start_time": "2021-05-25T00:56:42.483Z"
    }
   },
   "outputs": [],
   "source": [
    "function constructTransitionMatrix(agent::Agent,a_policy)\n",
    "  @unpack π,agrid = agent\n",
    "\n",
    "  N = length(agrid)\n",
    "  S = length(π)\n",
    "\n",
    "  H = spzeros(N*S,N*S) #sparse matrix to save on space\n",
    "  for n_ in 1:N\n",
    "    for s in 1:S\n",
    "      i_ = n_+N*(s-1) # index for ia_,s\n",
    "      n = a_policy[n_,s]\n",
    "      for sprime in 1:S\n",
    "        i = n+N*(sprime-1)\n",
    "        #probability of transitioning from state ia_,s to ia,sprime\n",
    "        H[i_,i] = π[sprime]\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  return H\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a function `find_stationary_distribution` that updates the transition matrix given a uniform initial distribution until it converges to a stationary distribution (represented by a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:56:49.892000-07:00",
     "start_time": "2021-05-25T00:56:49.886Z"
    }
   },
   "outputs": [],
   "source": [
    "function find_stationary_distribution(agent::Agent,a_policy)\n",
    "  H = constructTransitionMatrix(agent,a_policy)\n",
    "  N = size(H)[1]\n",
    "  π0 = ones(1,N)/N\n",
    "  diff = 1.\n",
    "  while diff > 1e-10\n",
    "    π1 = π0*H\n",
    "    diff = norm(π1-π0,Inf)\n",
    "    π0 = π1\n",
    "  end\n",
    "\n",
    "  return π0\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a function `computeassetdemand` that solves for the asset demand in the stationary equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:57:01.684000-07:00",
     "start_time": "2021-05-25T00:57:01.676Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  computeassetdemand(agent::Agent,r)\n",
    "\n",
    "Computes asset demanded in steady state at interest rate r and\n",
    "wage w\n",
    "\"\"\"\n",
    "function computeassetdemand(agent::Agent,r,w)\n",
    "  agent.r = r\n",
    "  agent.w = w\n",
    "\n",
    "  N = length(agent.agrid)\n",
    "  S = length(agent.e)\n",
    "  V0 = zeros(N,S)\n",
    "\n",
    "  V,a_policy = solveBellman_alt(agent,V0)\n",
    "  λss = find_stationary_distribution(agent,a_policy)\n",
    "\n",
    "  A = 0.\n",
    "  for n in 1:N\n",
    "    for s in 1:S\n",
    "      i = n + (s-1)*N\n",
    "      A  = A + λss[i] * agent.agrid[n]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  λss = reshape(λss,N,S) #λss[ia_,s] is the fraction of agents with assets agrid[ia]\n",
    "  #and productivity e[s]\n",
    "\n",
    "  return dot(agent.agrid,sum(λss,dims=2)) #agrid*πss gives average assets for each productivity\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T17:57:59.287000-07:00",
     "start_time": "2021-05-25T00:57:58.221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00042094784010208216"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(2.,0.96,.02,1.,[.8,1.,1.2],ones(3)/3,LinRange(-.5,25.,1000))\n",
    "computeassetdemand(agent,.03435,1.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.4",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
